{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "s9v88WCnshVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer_for_bert = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "metadata": {
        "id": "TAGYHcF_fv0F"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_question_answer(question, passage, max_len=512):\n",
        "    #Tokenize input question and passage\n",
        "    #Add special tokens - [CLS] and [SEP]\n",
        "    input_ids = tokenizer_for_bert.encode (question, passage,  max_length= max_len, truncation=True)\n",
        "\n",
        "\n",
        "    #Getting number of tokens in 1st sentence (question) and 2nd sentence (passage that contains answer)\n",
        "    sep_index = input_ids.index(102)\n",
        "    len_question = sep_index + 1\n",
        "    len_passage = len(input_ids)- len_question\n",
        "\n",
        "\n",
        "    #Need to separate question and passage\n",
        "    #Segment ids will be 0 for question and 1 for passage\n",
        "    segment_ids =  [0]*len_question + [1]*(len_passage)\n",
        "\n",
        "    #Converting token ids to tokens\n",
        "    tokens = tokenizer_for_bert.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    #Getting start and end scores for answer\n",
        "    #Converting input arrays to torch tensors before passing to the model\n",
        "    start_token_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]) )[0]\n",
        "    end_token_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]) )[1]\n",
        "\n",
        "    #Converting scores tensors to numpy arrays\n",
        "    start_token_scores = start_token_scores.detach().numpy().flatten()\n",
        "    end_token_scores = end_token_scores.detach().numpy().flatten()\n",
        "\n",
        "    #Getting start and end index of answer based on highest scores\n",
        "    answer_start_index = np.argmax(start_token_scores)\n",
        "    answer_end_index = np.argmax(end_token_scores)\n",
        "\n",
        "\n",
        "    #Getting scores for start and end token of the answer\n",
        "    start_token_score = np.round(start_token_scores[answer_start_index], 2)\n",
        "    end_token_score = np.round(end_token_scores[answer_end_index], 2)\n",
        "\n",
        "\n",
        "    #Combining subwords starting with ## and get full words in output.\n",
        "    #It is because tokenizer breaks words which are not in its vocab.\n",
        "    answer = tokens[answer_start_index]\n",
        "    for i in range(answer_start_index + 1, answer_end_index + 1):\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    # If the answer didn't find in the passage\n",
        "    if ( answer_start_index == 0) or (start_token_score < 0 ) or  (answer == '[SEP]') or ( answer_end_index <  answer_start_index):\n",
        "        answer = \"Sorry!, I could not find an answer in the passage.\"\n",
        "\n",
        "    return (answer_start_index, answer_end_index, start_token_score, end_token_score,  answer)\n"
      ],
      "metadata": {
        "id": "4w4fVy39f0uv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "id": "bXgFkBdngb9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "# Directory containing the PDF files\n",
        "directory_path = '/content/drive/MyDrive/BOOK'\n",
        "\n",
        "# List to store the extracted text from all PDF files\n",
        "all_text = []\n",
        "\n",
        "# Iterate through the PDF files in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Open the PDF file\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            # Create a PDF reader object\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "            # Extract text from each page\n",
        "            pages_text = []\n",
        "            for page in reader.pages:\n",
        "                pages_text.append(page.extract_text())\n",
        "\n",
        "            # Concatenate all the page text into a single string\n",
        "            pdf_text = ' '.join(pages_text)\n",
        "\n",
        "            # Append the PDF text to the list\n",
        "            all_text.append(pdf_text)\n",
        "\n",
        "# Split the text into words using regular expressions\n",
        "words = re.findall(r'\\b\\w+\\b', ' '.join(all_text))\n",
        "\n",
        "# Remove extra spaces and special characters from words\n",
        "cleaned_words = [re.sub(r'\\s+', ' ', word) for word in words]\n",
        "cleaned_words = [re.sub(r'[^\\w\\s]', '', word) for word in cleaned_words]\n",
        "\n",
        "# Store the cleaned words in a text file\n",
        "txt_path = '/content/drive/MyDrive/BOOK/PDF/extracted_words3.txt'\n",
        "with open(txt_path, 'w') as file:\n",
        "    for word in cleaned_words:\n",
        "        file.write(word + ' ')\n",
        "\n",
        "# Store the cleaned words in a variable\n",
        "text = ' '.join(cleaned_words)\n"
      ],
      "metadata": {
        "id": "-7GRq6eDgK5X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFhIeafZhVqR",
        "outputId": "48348a9e-4439-46e0-de58-6b3da2de444a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2635706"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPtkq_nZhKES",
        "outputId": "0005cb16-5ed4-4902-aa37-50792674c15d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Split the passage into smaller chunks based on word count with overlap\n",
        "chunk_size = 512\n",
        "overlap = 20\n",
        "texts = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - overlap)]\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Compute the TF-IDF matrix for the text chunks\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Convert the query chunk to a TF-IDF vector\n",
        "query_tfidf = vectorizer.transform([query_chunk])\n",
        "\n",
        "# Compute the cosine similarity between the query chunk and text chunks\n",
        "similarity_scores = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "\n",
        "# Sort the similarity scores in descending order\n",
        "sorted_indices = similarity_scores.argsort()[0][::-1]\n",
        "\n",
        "# Select the top similar chunks\n",
        "top_similar_chunks = [texts[idx] for idx in sorted_indices[:5]]\n",
        "\n",
        "# Load the similar chunks into separate variables\n",
        "neighbor1 = top_similar_chunks[0]\n",
        "neighbor2 = top_similar_chunks[1]\n",
        "neighbor3 = top_similar_chunks[2]\n",
        "neighbor4 = top_similar_chunks[3]\n",
        "neighbor5 = top_similar_chunks[4]\n",
        "\n",
        "# Print the top similar chunks\n",
        "print(\"Top Similar Chunks:\")\n",
        "for i, chunk in enumerate(top_similar_chunks, 1):\n",
        "    print(f\"Chunk {i}:\", chunk)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqu37oX-hSdc",
        "outputId": "c49bc93d-7c12-442b-e530-4113b090b5cf"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Similar Chunks:\n",
            "Chunk 1: milar tools to compile your Python code into fast optimized machine code Andriy Burkov The Hundred Page Machine Learning Book Draft 11 TheHundred PageMachineLearningBookAndriy Burkov All models are wrong but some are useful George Box The book is distributed on the read ﬁrst buy later principle Andriy BurkovThe Hundred Page Machine Learning Book Draft 9 Unsupervised Learning Unsupervised learning deals with problems in which your dataset doesn t have labels This property is what makes it very problematic fo\n",
            "Chunk 2: eated using the dataset of people could take as input a feature vector describing a person and output a probability that the person has cancer 1In this book if a term is in bold t h a tm e a n st h a tt h i st e r mc a nb ef o u n di nt h ei n d e xa tt h ee n do ft h e book Andriy Burkov The Hundred Page Machine Learning Book Draft 3 1 2 2 Unsupervised Learning Inunsupervised learning the dataset is a collection of unlabeled examples xi N i 1 Again xis a feature vector and the goal of an unsupervised learn\n",
            "Chunk 3: ture be handcrafted by humans or generated by another algorithm Machine learning can also be deﬁned as the process of solving a practical problem by 1 gathering a dataset and 2 algorithmically building a statistical model based on that dataset That statistical model is assumed to be used somehow to solve the practical problem To save keystrokes I use the terms learning and machine learning interchangeably 1 2 Types of Learning Learning can be supervised semi supervised unsupervised and reinforcement 1 2 1 S\n",
            "Chunk 4: ould ﬁnd in a bunch of books each thousand page thick Much of what I covered is not in the books at all typical machine learning books are conservative and academic while I emphasize those algorithms and methods that you will ﬁnd useful in your day to day work What exactly I didn t cover but would have covered if it was a thousand page machine learning book 11 1 Topic Modeling In text analysis topic modeling is a prevalent unsupervised learning problem You have a collection of text documents and you would l\n",
            "Chunk 5: ndard deviation from the mean Standard scores or z scores of features are calculated as follows ˆx j x j µ j j You may ask when you should use normalization and when standardization There s no deﬁnitive answer to this question Usually if your dataset is not too big and you have time you can try both and see which one performs better for your task If you don t have time to run multiple experiments as a rule of thumb Andriy Burkov The Hundred Page Machine Learning Book Draft 5 unsupervised learning algorithms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_chunk = \"what is unsupervised learning?\""
      ],
      "metadata": {
        "id": "IDGv_WCylvD0"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('\\nQuestion 1:\\n', query_chunk)\n",
        "_, _ , _ , _, ans  = bert_question_answer( query_chunk, neighbor1)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeMDL10LhtdT",
        "outputId": "67c661af-64c0-4fd5-8f69-476643492dea"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 1:\n",
            " what is unsupervised learning?\n",
            "\n",
            "Answer from BERT:  deals with problems in which your dataset doesn t have labels \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarized_chunks = []\n",
        "\n",
        "# Load the summarization pipeline with default max_length and min_length\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Summarize each top similar chunk\n",
        "for i, chunk in enumerate(top_similar_chunks, 1):\n",
        "    summarized_chunk = summarizer(chunk)[0]['summary_text']\n",
        "    summarized_chunks.append(summarized_chunk)\n",
        "    # print(f\"Neighbor {i} Summarized:\")\n",
        "    # print(summarized_chunk)\n",
        "\n",
        "# Store the summarized chunks in separate variables\n",
        "neighbor1 = summarized_chunks[0]\n",
        "neighbor2 = summarized_chunks[1]\n",
        "neighbor3 = summarized_chunks[2]\n",
        "neighbor4 = summarized_chunks[3]\n",
        "neighbor5 = summarized_chunks[4]\n",
        "# Concatenate the summarized chunks with a separator\n",
        "summary = ' '.join(summarized_chunks[:5])[:512]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VW_M2ici4dQ",
        "outputId": "8b38c04a-9f71-4d26-927c-20a0e8ad8714"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 142, but your input_length is only 98. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
            "Your max_length is set to 142, but your input_length is only 107. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n",
            "Your max_length is set to 142, but your input_length is only 102. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n",
            "Your max_length is set to 142, but your input_length is only 106. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n",
            "Your max_length is set to 142, but your input_length is only 98. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print ('\\nQuestion 1:\\n', query_chunk)\n",
        "_, _ , _ , _, ans  = bert_question_answer( query_chunk, summary)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbovw3bilcWl",
        "outputId": "86ea13e2-eaa1-4475-b8f7-fd13f2f2a587"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 1:\n",
            " what is suppervised learning?\n",
            "\n",
            "Answer from BERT:  supervised learning is the type of machine learning most frequently used in practice . the data for supervised learning is a collection of pairs input output \n",
            "\n"
          ]
        }
      ]
    }
  ]
}